{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024189bc",
   "metadata": {},
   "source": [
    "- Keras = th∆∞ vi·ªán Python d·ªÖ d√πng ƒë·ªÉ x√¢y d·ª±ng v√† hu·∫•n luy·ªán m·∫°ng neural (Deep Learning).\n",
    "- TensorFlow c√≥ th·ªÉ ho·∫°t ƒë·ªông nh∆∞ ‚Äúbackend‚Äù cho Keras, nh∆∞ng TensorFlow kh√¥ng ch·ªâ l√† backend, n√≥ l√† m·ªôt framework h·ªçc m√°y to√†n di·ªán.\n",
    "    - L√† m·ªôt framework m√£ ngu·ªìn m·ªü c·ªßa Google, d√πng ƒë·ªÉ x√¢y d·ª±ng v√† hu·∫•n luy·ªán c√°c m√¥ h√¨nh machine learning v√† deep learning.\n",
    "    - H·ªó tr·ª£ t√≠nh to√°n ma tr·∫≠n (tensors), GPU/TPU acceleration, distributed computing.\n",
    "    - C√≥ th·ªÉ d√πng TensorFlow API tr·ª±c ti·∫øp ƒë·ªÉ x√¢y d·ª±ng neural network (kh√¥ng c·∫ßn Keras).\n",
    "+ K·ªÉ t·ª´ TensorFlow 2.x, Keras ƒë√£ ƒë∆∞·ª£c t√≠ch h·ª£p v√†o TensorFlow d∆∞·ªõi d·∫°ng tf.keras.\n",
    "‚Üí B·∫°n kh√¥ng c·∫ßn c√†i Keras ri√™ng, m√† d√πng lu√¥n tf.keras.\n",
    "\n",
    "- Google (Keras, TensorFlow), \n",
    "- Facebook (Caffe2, Pytorch)\n",
    "- Microsoft (CNTK)\n",
    "- Amazon (Mxnet)\n",
    "\n",
    "- T√≥m t·∫Øt: Ch·ªçn th∆∞ vi·ªán n√†o?\n",
    "    - Nghi√™n c·ª©u: PyTorch, JAX\n",
    "    - ·ª®ng d·ª•ng c√¥ng nghi·ªáp: TensorFlow, PyTorch\n",
    "    - H·ªçc nhanh, prototype: Keras, Fastai\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9680a1",
   "metadata": {},
   "source": [
    "### Keras\n",
    "\n",
    "Vi·ªác hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh deep learning hay neural network n√≥i chung bao g·ªìm c√°c b∆∞·ªõc:\n",
    "1. Chu·∫©n b·ªã d·ªØ li·ªáu\n",
    "2. X√¢y d·ª±ng network\n",
    "3. Ch·ªçn thu·∫≠t to√°n c·∫≠p nh·∫≠t nghi·ªám, x√¢y d·ª±ng loss v√† ph∆∞∆°ng ph√°p ƒë√°nh gi√° m√¥ h√¨nh\n",
    "4. Hu·∫•n luy·ªán m√¥ h√¨nh.\n",
    "5. ƒê√°nh gi√° m√¥ h√¨nh\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c044ad9",
   "metadata": {},
   "source": [
    "- Backpropagation: \n",
    "    - 1 nh√† khoa h·ªçc ƒë√£ ch·ª©ng minh ƒëc neural nets v·ªõi nhi·ªÅu hidden layer (ƒë∆∞·ª£c g·ªçi l√† multi-layer perceptron ho·∫∑c MLP) c√≥ th·ªÉ ƒë∆∞·ª£c hu·∫•n luy·ªán m·ªôt c√°ch hi·ªáu qu·∫£ d·ª±a tr√™n m·ªôt quy tr√¨nh ƒë∆°n gi·∫£n ƒë∆∞·ª£c g·ªçi l√† backpropagation \n",
    "    - backpropagation l√† t√™n g·ªçi m·ªπ mi·ªÅu c·ªßa quy t·∫Øc chu·ªói ‚Äì chain rule ‚Äì trong t√≠nh ƒë·∫°o h√†m.\n",
    "    - Vi·ªác t√≠nh ƒë∆∞·ª£c ƒë·∫°o h√†m c·ªßa h√†m s·ªë ph·ª©c t·∫°p m√¥ t·∫£ quan h·ªá gi·ªØa ƒë·∫ßu v√†o v√† ƒë·∫ßu ra c·ªßa m·ªôt neural net l√† r·∫•t quan tr·ªçng v√¨ h·∫ßu h·∫øt c√°c thu·∫≠t to√°n t·ªëi ∆∞u ƒë·ªÅu ƒë∆∞·ª£c th·ª±c hi·ªán th√¥ng qua vi·ªác t√≠nh ƒë·∫°o h√†m, gradient descent l√† m·ªôt v√≠ d·ª•\n",
    "\n",
    "    - Vi·ªác n√†y gi√∫p neural nets tho√°t ƒë∆∞·ª£c nh·ªØng h·∫°n ch·∫ø c·ªßa perceptron v·ªÅ vi·ªác ch·ªâ bi·ªÉu di·ªÖn ƒë∆∞·ª£c c√°c quan h·ªá tuy·∫øn t√≠nh\n",
    "    - ƒê·ªÉ bi·ªÉu di·ªÖn c√°c quan h·ªá phi tuy·∫øn, ph√≠a sau m·ªói layer l√† m·ªôt h√†m k√≠ch ho·∫°t phi tuy·∫øn, v√≠ d·ª• h√†m sigmoid ho·∫∑c tanh. (ReLU ra ƒë·ªùi nƒÉm 2012). \n",
    "    - V·ªõi hidden layers, neural nets ƒë∆∞·ª£c ch·ª©ng minh r·∫±ng c√≥ kh·∫£ nƒÉng x·∫•p x·ªâ h·∫ßu h·∫øt b·∫•t k·ª≥ h√†m s·ªë n√†o qua m·ªôt ƒë·ªãnh l√Ω ƒë∆∞·ª£c g·ªçi l√† universal approximation theorem. \n",
    "    - Thu·∫≠t to√°n n√†y mang l·∫°i m·ªôt v√†i th√†nh c√¥ng ban ƒë·∫ßu CNN: nh·∫≠n d·∫°ng ch·ªØ s·ªë vi·∫øt tay\n",
    "- vanishing gradient: \n",
    "    - Khi h√†m m·∫•t m√°t ko ph·∫£i h√†m l·ªìi \n",
    "    - Khi ƒë√≥ t√¨m nghi·ªám to√†n c·ª•c cho b√†i to√°n t·ªëi ∆∞u h√†m m·∫•t m√°t tr·ªü n√™n kh√≥ khƒÉn \n",
    "    - Gi·ªõi h·∫°n t√≠nh to√°n c·ªßa m√°y t√≠nh c≈©ng khi·∫øn cho vi·ªác hu·∫•n luy·ªán MLP ko hi·ªáu qu·∫£ khi s·ªë l∆∞·ª£ng hidden layers l·ªõn l√™n \n",
    "\n",
    "    - Khi s·ª≠ d·ª•ng backpropagation ƒë·ªÉ t√≠nh ƒë·∫°o h√†m cho c√°c ma tr·∫≠n h·ªá s·ªë ·ªü c√°c l·ªõp ƒë·∫ßu ti√™n, ta c·∫ßn ph·∫£i nh√¢n r·∫•t nhi·ªÅu c√°c gi√° tr·ªã nh·ªè h∆°n 1 v·ªõi nhau\n",
    "    - Vi·ªác n√†y khi·∫øn cho nhi·ªÅu ƒë·∫°o h√†m th√†nh ph·∫ßn b·∫±ng 0 do x·∫•p x·ªâ t√≠nh to√°n\n",
    "    - Khi ƒë·∫°o h√†m c·ªßa m·ªôt th√†nh ph·∫ßn b·∫±ng 0, n√≥ s·∫Ω kh√¥ng ƒë∆∞·ª£c c·∫≠p nh·∫≠t th√¥ng qua gradient descent!\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d91b02",
   "metadata": {},
   "source": [
    "- MLP: Multi-Layer Perceptron\n",
    "- CNN (Convolutional Neural Nets - LeNet)\n",
    "- DBN: Deep Belief Nets \n",
    "- RNN (Recurrent Neural Network) ‚Äì M·∫°ng n∆°-ron h·ªìi ti·∫øp\n",
    "    - RNN d√πng ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu theo chu·ªói (sequence data)\n",
    "    - √ù t∆∞·ªüng: k·∫øt n·ªëi c√°c b∆∞·ªõc th·ªùi gian v·ªõi nhau, gi·ªØ th√¥ng tin ‚Äúng·∫Øn h·∫°n‚Äù t·ª´ b∆∞·ªõc tr∆∞·ªõc\n",
    "    - ∆Øu ƒëi·ªÉm:\n",
    "        - D·ªÖ c√†i ƒë·∫∑t, concept ƒë∆°n gi·∫£n  \n",
    "        - X·ª≠ l√Ω d·ªØ li·ªáu tu·∫ßn t·ª±\n",
    "    - Nh∆∞·ª£c ƒëi·ªÉm:\n",
    "        - Kh√≥ h·ªçc long-term dependencies (d·ªØ li·ªáu qu√° d√†i)\n",
    "        - D·ªÖ b·ªã vanishing gradient\n",
    "        - Vanishing gradient = hi·ªán t∆∞·ª£ng gradient tr·ªü n√™n qu√° nh·ªè khi lan truy·ªÅn ng∆∞·ª£c (backpropagation) qua nhi·ªÅu layer ho·∫∑c nhi·ªÅu b∆∞·ªõc th·ªùi gian, d·∫´n ƒë·∫øn vi·ªác m·∫°ng h·ªçc ch·∫≠m ho·∫∑c kh√¥ng h·ªçc ƒë∆∞·ª£c th√¥ng tin d√†i h·∫°n.\n",
    "- DNN (Deep Neural Network) - M·∫°ng s√¢u\n",
    "    - C·∫•u tr√∫c:\n",
    "        - G·ªìm nhi·ªÅu layer fully connected (dense layer)\n",
    "        - Input ‚Üí hidden layers ‚Üí output\n",
    "        - Kh√¥ng c√≥ k·∫øt n·ªëi ‚Äúquay l·∫°i‚Äù gi·ªØa c√°c b∆∞·ªõc\n",
    "    - D·ªØ li·ªáu x·ª≠ l√Ω:\n",
    "        - Static input (kh√¥ng theo chu·ªói)\n",
    "        - V√≠ d·ª•: d·ª± ƒëo√°n gi√° nh√† d·ª±a tr√™n s·ªë ph√≤ng, di·ªán t√≠ch, v·ªã tr√≠\n",
    "    - ∆Øu ƒëi·ªÉm:\n",
    "        - M·∫°nh cho c√°c b√†i to√°n classification, regression v·ªõi d·ªØ li·ªáu c·ªë ƒë·ªãnh\n",
    "        - Hu·∫•n luy·ªán ƒë∆°n gi·∫£n h∆°n RNN\n",
    "    - Nh∆∞·ª£c ƒëi·ªÉm:\n",
    "        - Kh√¥ng x·ª≠ l√Ω t·ªët sequence / d·ªØ li·ªáu theo th·ªùi gian\n",
    "        - Kh√¥ng gi·ªØ tr·∫°ng th√°i (state) t·ª´ b∆∞·ªõc tr∆∞·ªõc\n",
    "- LSTM (Long Short-Term Memory)\n",
    "    - L√† phi√™n b·∫£n c·∫£i ti·∫øn c·ªßa RNN, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ gi·ªØ th√¥ng tin l√¢u h∆°n.\n",
    "    - C√≥ gate (c·ªïng) ƒë·ªÉ quy·∫øt ƒë·ªãnh:\n",
    "        - C√°i g√¨ n√™n l∆∞u (input gate)\n",
    "        - C√°i g√¨ n√™n qu√™n (forget gate)\n",
    "        - C√°i g√¨ n√™n xu·∫•t (output gate)\n",
    "    - ∆Øu ƒëi·ªÉm:\n",
    "        - Gi·ªØ ƒë∆∞·ª£c th√¥ng tin d√†i h·∫°n\n",
    "        - Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ vanishing gradient c·ªßa RNN\n",
    "    - Nh∆∞·ª£c ƒëi·ªÉm:\n",
    "        - Ph·ª©c t·∫°p h∆°n RNN, t√≠nh to√°n nhi·ªÅu h∆°n\n",
    "- GRU (Gated Recurrent Unit)\n",
    "    - L√† bi·∫øn th·ªÉ nh·∫π h∆°n LSTM, k·∫øt h·ª£p forget + input gate th√†nh update gate.\n",
    "    - √çt tham s·ªë h∆°n LSTM ‚Üí hu·∫•n luy·ªán nhanh h∆°n\n",
    "    - ∆Øu ƒëi·ªÉm:\n",
    "        - T·ªët cho d·ªØ li·ªáu tu·∫ßn t·ª± ng·∫Øn v√† trung b√¨nh\n",
    "        - Nhanh h∆°n LSTM\n",
    "        - Hi·ªáu nƒÉng t∆∞∆°ng ƒë∆∞∆°ng LSTM nhi·ªÅu tr∆∞·ªùng h·ª£p\n",
    "---- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a5dce",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "1. H√†m sigmoid: \n",
    "2. H√†m tanh: ƒê√¢y l√† m·ªôt h√†m phi tuy·∫øn (non-linear) bi·∫øn ƒë·ªïi gi√° tr·ªã ƒë·∫ßu v√†o th√†nh gi√° tr·ªã ƒë·∫ßu ra n·∫±m trong kho·∫£ng (‚àí1,1).\n",
    "$$\n",
    "\\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "- ·ª®ng d·ª•ng\n",
    "    - Trong m·∫°ng n∆°-ron nh√¢n t·∫°o, h√†m tanh th∆∞·ªùng d√πng l√†m h√†m k√≠ch ho·∫°t (activation function) nh·ªù kh·∫£ nƒÉng chu·∫©n h√≥a d·ªØ li·ªáu v√†o kho·∫£ng [‚àí1,1], gi√∫p qu√° tr√¨nh h·ªçc nhanh h∆°n so v·ªõi h√†m sigmoid (n·∫±m trong kho·∫£ng [0,1]).\n",
    "\n",
    "    - Trong x·ª≠ l√Ω t√≠n hi·ªáu, gi√∫p chuy·ªÉn ƒë·ªïi t√≠n hi·ªáu tuy·∫øn t√≠nh th√†nh phi tuy·∫øn ƒë·ªÉ m√¥ ph·ªèng c√°c h·ªá th·ªëng phi tuy·∫øn.\n",
    "3. Relu: Rectified Linear Unit: ƒê∆°n gi·∫£n v√† hi·ªáu qu·∫£: T√≠nh to√°n nhanh, gi√∫p gi·∫£m v·∫•n ƒë·ªÅ vanishing gradient (gradient bi·∫øn m·∫•t) th∆∞·ªùng g·∫∑p v·ªõi sigmoid ho·∫∑c tanh.\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "- N·∫øu ùë•> 0 ƒë·∫ßu ra = ùë•\n",
    "- N·∫øu ùë• ‚â§ 0 ƒë·∫ßu ra = 0\n",
    "\n",
    "4. Softmax: d√πng ·ªü output layer cho c√°c b√†i to√°n ph√¢n lo·∫°i nhi·ªÅu l·ªõp (multi-class classification).\n",
    "$$\n",
    "\\mathrm{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "- logits = [2, 1, 0]: l√† ƒë·∫ßu ra tr∆∞·ªõc khi qua h√†m k√≠ch ho·∫°t \n",
    "- t√≠nh $ e^{z_i} $: $ e^{2} = 7.39, e^{1} = 2.72, e^{0} = 1$ => t·ªïng = 11.11\n",
    "- T·ªïng c√°c output = 1\n",
    "- C√≥ th·ªÉ hi·ªÉu l√† x√°c su·∫•t c·ªßa t·ª´ng l·ªõp (chia cho t) t∆∞∆°ng ·ª©ng: \n",
    "    - 7.39 / 11.11 ‚âà 0.665\n",
    "    - 2.72 / 11.11 ‚âà 0.245\n",
    "    - 1 / 11.11 ‚âà 0.090\n",
    "\n",
    "- Softmax c√≥ th·ªÉ hi·ªÉu l√†: M√¥ h√¨nh gi·∫£ ƒë·ªãnh logits l√† log c·ªßa x√°c su·∫•t ch∆∞a chu·∫©n h√≥a\n",
    "$$\n",
    "z_i = \\log \\tilde{P}(y=i \\mid x)\n",
    "$$\n",
    "$$\\tilde{P}(y=i \\mid x) = e^{z_i}$$\n",
    "\n",
    "\n",
    "==> Sigmoid v√† tanh ph·ªï bi·∫øn trong qu√° kh·ª©, nh∆∞ng hi·ªán nay √≠t ƒëc sd v√¨ khi ƒë·∫ßu v√†o l·ªõn, th√¨ ƒë·∫°o h√†m ti·ªám c·∫≠n v·ªÅ 0 => h·ªá s·ªë t∆∞∆°ng ·ª©ng v·ªõi c√°c unit ƒëang x√©t ko ƒëc c·∫≠p nh·∫≠t (kill gradients)\n",
    "=> Relu v√† softmax th∆∞·ªùng ƒëc sd h∆°n\n",
    "\n",
    "- Trong 1 network, ko c·∫ßn h√†m activate ph·∫£i gi·ªëng nhau, nh∆∞ng th∆∞·ªùng ƒëc khuy·∫øn ngh·ªã l√† gi·ªëng nhau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee95061",
   "metadata": {},
   "source": [
    "### Epoch\n",
    "- Epoch: M·ªôt epoch l√† m·ªôt v√≤ng l·∫∑p ƒë·∫ßy ƒë·ªß qua to√†n b·ªô t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán.\n",
    "    - N·∫øu b·∫°n c√≥ 1.000 m·∫´u d·ªØ li·ªáu v√† m√¥ h√¨nh xem h·∫øt c·∫£ 1.000 m·∫´u m·ªôt l·∫ßn, th√¨ ƒë√≥ l√† 1 epoch.\n",
    "    - Sau m·ªói epoch, m√¥ h√¨nh c·∫≠p nh·∫≠t tr·ªçng s·ªë (weights) d·ª±a tr√™n l·ªói (loss) t√≠nh ƒë∆∞·ª£c t·ª´ to√†n b·ªô d·ªØ li·ªáu.\n",
    "- Batch: D·ªØ li·ªáu th∆∞·ªùng ƒë∆∞·ª£c chia th√†nh c√°c ph·∫ßn nh·ªè g·ªçi l√† batch. M·ªôt batch ƒëi qua m√¥ h√¨nh l√† m·ªôt b∆∞·ªõc (step).\n",
    "- Iterations (s·ªë b∆∞·ªõc l·∫∑p): N·∫øu b·∫°n c√≥ 1.000 m·∫´u d·ªØ li·ªáu v√† batch size = 100, th√¨ m·ªôt epoch s·∫Ω g·ªìm 10 iterations.\n",
    "- Vai tr√≤ c·ªßa epoch: \n",
    "    - Epoch qu√° th·∫•p ‚Üí m√¥ h√¨nh ch∆∞a h·ªçc ƒë·ªß, k·∫øt qu·∫£ k√©m (underfitting).\n",
    "    - Epoch qu√° cao ‚Üí m√¥ h√¨nh h·ªçc qu√° k·ªπ d·ªØ li·ªáu hu·∫•n luy·ªán, m·∫•t kh·∫£ nƒÉng t·ªïng qu√°t (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974c632",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Units of hidden layers: \n",
    "\n",
    "N·∫øu hidden layer c√≥ 64 units th√¨ nghƒ©a l√†:\n",
    "- T·∫ßng ƒë√≥ c√≥ 64 neuron (n√∫t)\n",
    "- M·ªói neuron:\n",
    "    - Nh·∫≠n to√†n b·ªô input t·ª´ layer tr∆∞·ªõc\n",
    "    - Nh√¢n v·ªõi tr·ªçng s·ªë (weights)\n",
    "    - C·ªông bias\n",
    "    - ƒêi qua h√†m k√≠ch ho·∫°t (ReLU, sigmoid, tanh‚Ä¶)\n",
    "\n",
    "==> s·ªë neuron c·ªßa layer n√†y s·∫Ω units c·ªßa layer ti·∫øp theo?\n",
    "\n",
    "==> M·ªói m·ªôt ƒë∆°n v·ªã s·∫Ω l√† k·∫øt qu·∫£ ƒë·∫°i di·ªán c·ªßa vi·ªác √°p d·ª•ng 1 b·ªô l·ªçc ƒë·ªÉ t√¨m ra m·ªôt ƒë·∫∑c tr∆∞ng c·ª• th·ªÉ\n",
    "\n",
    "### embedding vector: \n",
    "- Vector nh√∫ng (embedding vector) l√† m·ªôt vector s·ªë th·ª±c (float vector) m√† m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c ƒë·ªÉ bi·ªÉu di·ªÖn ƒë·ªëi t∆∞·ª£ng (word, token, item‚Ä¶) trong kh√¥ng gian li√™n t·ª•c, sao cho nh·ªØng ƒë·ªëi t∆∞·ª£ng gi·ªëng nhau s·∫Ω g·∫ßn nhau trong kh√¥ng gian n√†y.\n",
    "- ƒê·ªëi t∆∞·ª£ng c√≥ th·ªÉ l√†:\n",
    "    - T·ª´ (word embedding, NLP)\n",
    "    - Item / s·∫£n ph·∫©m (recommender system)\n",
    "    - Ng∆∞·ªùi d√πng (user embedding)\n",
    "    - Node / graph (graph embedding)\n",
    "    - Feature categorical (tabular embedding)\n",
    "\n",
    "### Loss function\n",
    "- H√†m loss ch√≠nh l√† cross entropy ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒëo l∆∞·ªùng s·ª± kh√°c bi·ªát c·ªßa ground truth v·ªõi ph√¢n ph·ªëi x√°c su·∫•t ƒë∆∞·ª£c d·ª± b√°o c√≥ c√¥ng th·ª©c nh∆∞ sau:\n",
    "- Loss c≈©ng d√πng ƒë·ªÉ c·∫≠p nh·∫≠t learning rate c·ªßa tham s·ªë theo gradient th√¥i \n",
    "\n",
    "### t√¨m ra s·ªë l∆∞·ª£ng units v√† h√†m activation ntn?\n",
    "- l√† b·∫•t kh·∫£ thi \n",
    "- Thay v√†o ƒë√≥ tƒÉng s·ªë l∆∞·ª£ng hidden layers k·∫øt h·ª£p v·ªõi c√°c nonlinear activation c√≥ kh·∫£ nƒÉng x·∫•p x·ªâ training data t·ªët h∆°n \n",
    "- Khi s·ªë l∆∞·ª£ng hidden layers l·ªõn l√™n -> s·ªë l∆∞·ª£ng h·ªá s·ªë c·∫ßn t·ªëi ∆∞u c≈©ng l·ªõn l√™n -> m√¥ h√¨nh ph·ª©c t·∫°p -> t·ªëc ƒë·ªô t√≠nh to√°n ch·∫≠m v√† c≈©ng d·ªÖ b·ªã overfitting \n",
    "- N·∫øu m·ªçi units c·ªßa 1 layer ƒëc k·∫øt n·ªëi v·ªõi m·ªçi unit c·ªßa layer ti·∫øp theo th√¨ ta g·ªçi l√† fully connected layer (Dense layer????). 1 network m√† to√†n fully connected layers th√¨ √≠t ƒëc sd trong th·ª±c t·∫ø. -> d√πng c√°c bi·ªán ph√°p kh√°c nhau l√†m gi·∫£m s·ª± ph·ª©c t·∫°p c·ªßa m√¥ h√¨nh b·∫±ng c√°ch s·ªë l∆∞·ª£ng k·∫øt n·ªëi vd: cho k·∫øt n·ªëi = 0 ho·∫∑c c√°c h·ªá s·ªë r√†ng bu·ªôc gi·ªëng nhau (ƒë·ªÉ gi·∫£m s·ªë l∆∞·ª£ng c√°c h·ªá s·ªë c·∫ßn t·ªëi ∆∞u)\n",
    "\n",
    "### 1Ô∏è‚É£ C√°c y·∫øu t·ªë c·∫ßn t·ªëi ∆∞u c·ªßa b√†i to√°n neural network \n",
    "- a) S·ªë layers (depth)\n",
    "    - M·∫°ng n√¥ng (few layers): h·ªçc ch·∫≠m, kh·∫£ nƒÉng tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng h·∫°n ch·∫ø\n",
    "    - M·∫°ng s√¢u (many layers): h·ªçc bi·ªÉu di·ªÖn ph·ª©c t·∫°p h∆°n, nh∆∞ng d·ªÖ overfit, kh√≥ hu·∫•n luy·ªán\n",
    "    - Th∆∞·ªùng test 2‚Äì5 layer trong Encoder v√† Decoder l√† ƒë·ªß cho d·ªØ li·ªáu v·ª´a v√† nh·ªè\n",
    "- b) S·ªë units m·ªói layer (width)\n",
    "    - M·ªói layer c√≥ bao nhi√™u neuron ‚Üí quy·∫øt ƒë·ªãnh kh·∫£ nƒÉng bi·ªÉu di·ªÖn c·ªßa layer\n",
    "        - Bottleneck qu√° nh·ªè ‚Üí m·∫•t th√¥ng tin\n",
    "        - Bottleneck qu√° l·ªõn ‚Üí autoencoder ch·ªâ copy input, kh√¥ng h·ªçc g√¨ m·ªõi\n",
    "- c) K√≠ch ho·∫°t (activation)\n",
    "    - ReLU, sigmoid, tanh‚Ä¶ ·∫£nh h∆∞·ªüng t·ªõi c√°ch m·∫°ng h·ªçc\n",
    "        - ReLU t·ªët cho d·ªØ li·ªáu real-value, sparse\n",
    "        - Softmax + categorical_crossentropy cho d·ªØ li·ªáu one-hot\n",
    "- d) Loss function & optimizer\n",
    "    - MSE, MAE, categorical_crossentropy‚Ä¶ t√πy ki·ªÉu d·ªØ li·ªáu\n",
    "    - Optimizer (Adam, RMSProp, SGD) ·∫£nh h∆∞·ªüng t·ªëc ƒë·ªô h·ªçc\n",
    "\n",
    "### 2Ô∏è‚É£ Quy tr√¨nh t·ªëi ∆∞u\n",
    "1. Ch·ªçn s·ªë layer v√† units ban ƒë·∫ßu theo kinh nghi·ªám ho·∫∑c th·ª≠ nghi·ªám s∆° b·ªô\n",
    "2. Hu·∫•n luy·ªán m·∫°ng tr√™n d·ªØ li·ªáu\n",
    "3. ƒê√°nh gi√°: Loss, reconstruction error, ho·∫∑c downstream task (v√≠ d·ª• accuracy khi d√πng latent vector cho recommendation)\n",
    "4. Th·ª≠ c√°c ki·∫øn tr√∫c kh√°c:\n",
    "    - Thay ƒë·ªïi s·ªë layer\n",
    "    - Thay ƒë·ªïi s·ªë neuron m·ªói layer\n",
    "    - Thay ƒë·ªïi activation, learning rate‚Ä¶\n",
    "5. Ch·ªçn ki·∫øn tr√∫c t·ªëi ∆∞u theo ti√™u ch√≠: gi√° tr·ªã loss nh·ªè nh·∫•t, latent vector ƒë·ªß n√©n nh∆∞ng v·∫´n gi·ªØ th√¥ng tin\n",
    "\n",
    "### L∆∞u √Ω\n",
    "-  Kh√¥ng c√≥ c√¥ng th·ª©c ‚Äúchu·∫©n‚Äù cho s·ªë layer hay neuron ‚Üí ph·∫£i th·ª≠ nghi·ªám (hyperparameter tuning)\n",
    "- Th∆∞·ªùng d√πng:\n",
    "    - Grid search / Random search / Bayesian optimization\n",
    "    - Early stopping ƒë·ªÉ tr√°nh overfit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20375ca5",
   "metadata": {},
   "source": [
    "### C√°c lo·∫°i t·ªëi ∆∞u (Optimizer)\n",
    "1. 1Ô∏è‚É£ Stochastic Gradient Descent (SGD)\n",
    "    - C∆° b·∫£n nh·∫•t: c·∫≠p nh·∫≠t weights d·ª±a tr√™n gradient c·ªßa loss function.\n",
    "        - w=w‚àíŒ∑‚ãÖ‚àáL\n",
    "        - Œ∑ = learning rate \n",
    "        - ‚àáL = gradient c·ªßa loss theo weights\n",
    "    - ∆Øu ƒëi·ªÉm: ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu\n",
    "    - Nh∆∞·ª£c ƒëi·ªÉm: ch·∫≠m, d·ªÖ r∆°i v√†o local minima, c·∫ßn tuning learning rate\n",
    "    - Th∆∞·ªùng d√πng: v·ªõi momentum ƒë·ªÉ c·∫£i thi·ªán t·ªëc ƒë·ªô\n",
    "2. 2Ô∏è‚É£ SGD v·ªõi Momentum: Th√™m momentum ƒë·ªÉ ‚Äúnh·ªõ h∆∞·ªõng ƒëi tr∆∞·ªõc‚Äù ‚Üí tr√°nh rung l·∫Øc trong valley c·ªßa loss surface.\n",
    "    - v=Œ≤v+Œ∑‚àáL,w=w‚àív\n",
    "    - Œ≤ ~ 0.9, quy·∫øt ƒë·ªãnh m·ª©c ‚Äúnh·ªõ gradient c≈©‚Äù\n",
    "    - ∆Øu ƒëi·ªÉm: nhanh h∆°n SGD chu·∫©n, √≠t b·ªã stuck\n",
    "3. 3Ô∏è‚É£ RMSProp\n",
    "    - Chia learning rate theo b√¨nh ph∆∞∆°ng trung b√¨nh c·ªßa gradient g·∫ßn ƒë√¢y\n",
    "    - ∆Øu ƒëi·ªÉm: t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh learning rate, t·ªët v·ªõi d·ªØ li·ªáu sparse\n",
    "    - Th∆∞·ªùng d√πng: RNN, LSTM\n",
    "4. 4Ô∏è‚É£ Adam (Adaptive Moment Estimation)\n",
    "    - K·∫øt h·ª£p Momentum + RMSProp\n",
    "    - C·∫≠p nh·∫≠t weights d·ª±a tr√™n gradient trung b√¨nh (momentum) + b√¨nh ph∆∞∆°ng gradient (RMS)\n",
    "    - C√¥ng th·ª©c ph·ª©c t·∫°p, nh∆∞ng hi·ªáu qu·∫£ r·∫•t cao:\n",
    "    - ∆Øu ƒëi·ªÉm: nhanh, ·ªïn ƒë·ªãnh, √≠t c·∫ßn tune learning rate\n",
    "    - Nh∆∞·ª£c ƒëi·ªÉm: ƒë√¥i khi generalize k√©m h∆°n SGD v·ªõi momentum\n",
    "5. AdaGrad\n",
    "6. AdamW\n",
    "7. Nadam\n",
    "8. Kh√°c: \n",
    "    - Adadelta: c·∫£i ti·∫øn AdaGrad, tr√°nh gi·∫£m learning rate qu√° nhanh\n",
    "    - FTRL: th∆∞·ªùng d√πng cho m√¥ h√¨nh linear v·ªõi d·ªØ li·ªáu sparse l·ªõn\n",
    "    - L-BFGS: optimizer d·∫°ng quasi-Newton, hi·∫øm d√πng cho NN l·ªõn, t·ªët cho batch nh·ªè"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c34d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### C√°c l·∫°i layers ph·ªï bi·∫øn: \n",
    "1. Dense / Fully Connected Layer\n",
    "- M√¥ t·∫£: M·ªói neuron k·∫øt n·ªëi v·ªõi t·∫•t c·∫£ input t·ª´ layer tr∆∞·ªõc.\n",
    "- C√¥ng th·ª©c:\n",
    "    - z = Œ£ w_i * x_i + b, a = f(z)\n",
    "    - Trong ƒë√≥:\n",
    "        - x1, x2, ..., xn: c√°c input\n",
    "        - w1, w2, ..., wn: tr·ªçng s·ªë\n",
    "        - b: bias\n",
    "        - f: h√†m k√≠ch ho·∫°t\n",
    "        - a: output c·ªßa neuron\n",
    "- D√πng cho: Tabular data, cu·ªëi pipeline CNN/RNN.\n",
    "- Activation: ReLU, Sigmoid, Tanh, Softmax‚Ä¶\n",
    "2. Convolution (Conv) neuron (2D): \n",
    "- M√¥ t·∫£: D√πng kernel/filter qu√©t input (th∆∞·ªùng l√† h√¨nh ·∫£nh ho·∫∑c d·ªØ li·ªáu kh√¥ng gian).\n",
    "- C√¥ng th·ª©c c·ªßa m·ªôt Conv neuron t·∫°i v·ªã tr√≠ (i, j):\n",
    "    z(i,j) = Œ£ Œ£ W(m,n) * X(i+m, j+n) + b\n",
    "- D√πng cho: Image, video, text (CNN-based).\n",
    "- ƒê·∫∑c ƒëi·ªÉm: Weight sharing, receptive field, spatial invariance.\n",
    "3. Pooling Layer\n",
    "- M√¥ t·∫£: Gi·∫£m k√≠ch th∆∞·ªõc kh√¥ng gian, gi·ªØ th√¥ng tin quan tr·ªçng.\n",
    "- C√°c lo·∫°i: Max pooling, Average pooling, Global pooling.\n",
    "- D√πng cho: CNN, gi·∫£m overfitting, tƒÉng t·ªëc t√≠nh to√°n.\n",
    "4. Recurrent / RNN Layer\n",
    "- M√¥ t·∫£: D√πng cho d·ªØ li·ªáu tu·∫ßn t·ª± (sequence, text, time series).\n",
    "- C√°c lo·∫°i: Vanilla RNN, LSTM, GRU.\n",
    "- ƒê·∫∑c ƒëi·ªÉm: Nh·ªõ th√¥ng tin t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc (hidden state).\n",
    "5. Normalization Layer\n",
    "- M√¥ t·∫£: Chu·∫©n h√≥a output c·ªßa layer tr∆∞·ªõc, gi√∫p training ·ªïn ƒë·ªãnh.\n",
    "- C√°c lo·∫°i: BatchNorm, LayerNorm, InstanceNorm, GroupNorm.\n",
    "- C√¥ng d·ª•ng: Gi·∫£m vanishing/exploding gradient, tƒÉng t·ªëc h·ªôi t·ª•.\n",
    "6. Dropout / Regularization Layer\n",
    "- M√¥ t·∫£: T·∫°m th·ªùi ‚Äút·∫Øt‚Äù m·ªôt s·ªë neuron trong training.\n",
    "- C√¥ng d·ª•ng: Gi·∫£m overfitting, l√†m m√¥ h√¨nh t·ªïng qu√°t t·ªët h∆°n.\n",
    "7. Activation Layer\n",
    "- M√¥ t·∫£: Ch·ªâ th·ª±c hi·ªán h√†m phi tuy·∫øn.\n",
    "- C√°c lo·∫°i: ReLU, Sigmoid, Tanh, Softmax, Leaky ReLU, GELU‚Ä¶\n",
    "- D√πng cho: Hidden ho·∫∑c output layer.\n",
    "8. Embedding Layer\n",
    "- M√¥ t·∫£: Bi·∫øn categorical data ho·∫∑c words ‚Üí vectors.\n",
    "- D√πng cho: NLP, recommender systems.\n",
    "9. Flatten / Reshape Layer\n",
    "- M√¥ t·∫£: Bi·∫øn d·ªØ li·ªáu t·ª´ nhi·ªÅu chi·ªÅu sang vector 1D.\n",
    "- D√πng cho: Chu·∫©n b·ªã input t·ª´ Conv ‚Üí Dense layer.\n",
    "10. C√°c layer ƒë·∫∑c bi·ªát / m·ªõi\n",
    "- Attention layer: Transformer, ViT, NLP\n",
    "- Residual / Skip connection: ResNet, DenseNet\n",
    "- UpSampling / Deconvolution: GAN, segmentation\n",
    "- Concatenate / Add: K·∫øt h·ª£p nhi·ªÅu nh√°nh m·∫°ng\n",
    "\n",
    "\n",
    "### Tr·ª±c gi√°c d·ªÖ nh·ªõ\n",
    "- Dense neuron:\n",
    "üëâ ‚ÄúNh√¨n to√†n b·ªô b·ª©c ·∫£nh c√πng l√∫c‚Äù\n",
    "- Conv neuron:\n",
    "üëâ ‚ÄúNh√¨n t·ª´ng m·∫£nh nh·ªè v√† d√πng c√πng m·ªôt k√≠nh l√∫p‚Äù"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
