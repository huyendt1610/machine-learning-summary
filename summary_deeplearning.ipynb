{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024189bc",
   "metadata": {},
   "source": [
    "- Keras = thÆ° viá»‡n Python dá»… dÃ¹ng Ä‘á»ƒ xÃ¢y dá»±ng vÃ  huáº¥n luyá»‡n máº¡ng neural (Deep Learning).\n",
    "- TensorFlow cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng nhÆ° â€œbackendâ€ cho Keras, nhÆ°ng TensorFlow khÃ´ng chá»‰ lÃ  backend, nÃ³ lÃ  má»™t framework há»c mÃ¡y toÃ n diá»‡n.\n",
    "    - LÃ  má»™t framework mÃ£ nguá»“n má»Ÿ cá»§a Google, dÃ¹ng Ä‘á»ƒ xÃ¢y dá»±ng vÃ  huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh machine learning vÃ  deep learning.\n",
    "    - Há»— trá»£ tÃ­nh toÃ¡n ma tráº­n (tensors), GPU/TPU acceleration, distributed computing.\n",
    "    - CÃ³ thá»ƒ dÃ¹ng TensorFlow API trá»±c tiáº¿p Ä‘á»ƒ xÃ¢y dá»±ng neural network (khÃ´ng cáº§n Keras).\n",
    "+ Ká»ƒ tá»« TensorFlow 2.x, Keras Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p vÃ o TensorFlow dÆ°á»›i dáº¡ng tf.keras.\n",
    "â†’ Báº¡n khÃ´ng cáº§n cÃ i Keras riÃªng, mÃ  dÃ¹ng luÃ´n tf.keras.\n",
    "\n",
    "- Google (Keras, TensorFlow), \n",
    "- Facebook (Caffe2, Pytorch)\n",
    "- Microsoft (CNTK)\n",
    "- Amazon (Mxnet)\n",
    "\n",
    "- TÃ³m táº¯t: Chá»n thÆ° viá»‡n nÃ o?\n",
    "    - NghiÃªn cá»©u: PyTorch, JAX\n",
    "    - á»¨ng dá»¥ng cÃ´ng nghiá»‡p: TensorFlow, PyTorch\n",
    "    - Há»c nhanh, prototype: Keras, Fastai\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9680a1",
   "metadata": {},
   "source": [
    "### Keras\n",
    "\n",
    "Viá»‡c huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh deep learning hay neural network nÃ³i chung bao gá»“m cÃ¡c bÆ°á»›c:\n",
    "1. Chuáº©n bá»‹ dá»¯ liá»‡u\n",
    "2. XÃ¢y dá»±ng network\n",
    "3. Chá»n thuáº­t toÃ¡n cáº­p nháº­t nghiá»‡m, xÃ¢y dá»±ng loss vÃ  phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh\n",
    "4. Huáº¥n luyá»‡n mÃ´ hÃ¬nh.\n",
    "5. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c044ad9",
   "metadata": {},
   "source": [
    "- Backpropagation: \n",
    "    - 1 nhÃ  khoa há»c Ä‘Ã£ chá»©ng minh Ä‘c neural nets vá»›i nhiá»u hidden layer (Ä‘Æ°á»£c gá»i lÃ  multi-layer perceptron hoáº·c MLP) cÃ³ thá»ƒ Ä‘Æ°á»£c huáº¥n luyá»‡n má»™t cÃ¡ch hiá»‡u quáº£ dá»±a trÃªn má»™t quy trÃ¬nh Ä‘Æ¡n giáº£n Ä‘Æ°á»£c gá»i lÃ  backpropagation \n",
    "    - backpropagation lÃ  tÃªn gá»i má»¹ miá»u cá»§a quy táº¯c chuá»—i â€“ chain rule â€“ trong tÃ­nh Ä‘áº¡o hÃ m.\n",
    "    - Viá»‡c tÃ­nh Ä‘Æ°á»£c Ä‘áº¡o hÃ m cá»§a hÃ m sá»‘ phá»©c táº¡p mÃ´ táº£ quan há»‡ giá»¯a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra cá»§a má»™t neural net lÃ  ráº¥t quan trá»ng vÃ¬ háº§u háº¿t cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u Ä‘á»u Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua viá»‡c tÃ­nh Ä‘áº¡o hÃ m, gradient descent lÃ  má»™t vÃ­ dá»¥\n",
    "\n",
    "    - Viá»‡c nÃ y giÃºp neural nets thoÃ¡t Ä‘Æ°á»£c nhá»¯ng háº¡n cháº¿ cá»§a perceptron vá» viá»‡c chá»‰ biá»ƒu diá»…n Ä‘Æ°á»£c cÃ¡c quan há»‡ tuyáº¿n tÃ­nh\n",
    "    - Äá»ƒ biá»ƒu diá»…n cÃ¡c quan há»‡ phi tuyáº¿n, phÃ­a sau má»—i layer lÃ  má»™t hÃ m kÃ­ch hoáº¡t phi tuyáº¿n, vÃ­ dá»¥ hÃ m sigmoid hoáº·c tanh. (ReLU ra Ä‘á»i nÄƒm 2012). \n",
    "    - Vá»›i hidden layers, neural nets Ä‘Æ°á»£c chá»©ng minh ráº±ng cÃ³ kháº£ nÄƒng xáº¥p xá»‰ háº§u háº¿t báº¥t ká»³ hÃ m sá»‘ nÃ o qua má»™t Ä‘á»‹nh lÃ½ Ä‘Æ°á»£c gá»i lÃ  universal approximation theorem. \n",
    "    - Thuáº­t toÃ¡n nÃ y mang láº¡i má»™t vÃ i thÃ nh cÃ´ng ban Ä‘áº§u CNN: nháº­n dáº¡ng chá»¯ sá»‘ viáº¿t tay\n",
    "- vanishing gradient: \n",
    "    - Khi hÃ m máº¥t mÃ¡t ko pháº£i hÃ m lá»“i \n",
    "    - Khi Ä‘Ã³ tÃ¬m nghiá»‡m toÃ n cá»¥c cho bÃ i toÃ¡n tá»‘i Æ°u hÃ m máº¥t mÃ¡t trá»Ÿ nÃªn khÃ³ khÄƒn \n",
    "    - Giá»›i háº¡n tÃ­nh toÃ¡n cá»§a mÃ¡y tÃ­nh cÅ©ng khiáº¿n cho viá»‡c huáº¥n luyá»‡n MLP ko hiá»‡u quáº£ khi sá»‘ lÆ°á»£ng hidden layers lá»›n lÃªn \n",
    "\n",
    "    - Khi sá»­ dá»¥ng backpropagation Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m cho cÃ¡c ma tráº­n há»‡ sá»‘ á»Ÿ cÃ¡c lá»›p Ä‘áº§u tiÃªn, ta cáº§n pháº£i nhÃ¢n ráº¥t nhiá»u cÃ¡c giÃ¡ trá»‹ nhá» hÆ¡n 1 vá»›i nhau\n",
    "    - Viá»‡c nÃ y khiáº¿n cho nhiá»u Ä‘áº¡o hÃ m thÃ nh pháº§n báº±ng 0 do xáº¥p xá»‰ tÃ­nh toÃ¡n\n",
    "    - Khi Ä‘áº¡o hÃ m cá»§a má»™t thÃ nh pháº§n báº±ng 0, nÃ³ sáº½ khÃ´ng Ä‘Æ°á»£c cáº­p nháº­t thÃ´ng qua gradient descent!\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d91b02",
   "metadata": {},
   "source": [
    "- MLP: Multi-Layer Perceptron\n",
    "- CNN (Convolutional Neural Nets - LeNet)\n",
    "- DBN: Deep Belief Nets \n",
    "- RNN (Recurrent Neural Network) â€“ Máº¡ng nÆ¡-ron há»“i tiáº¿p\n",
    "    - RNN dÃ¹ng Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u theo chuá»—i (sequence data)\n",
    "    - Ã tÆ°á»Ÿng: káº¿t ná»‘i cÃ¡c bÆ°á»›c thá»i gian vá»›i nhau, giá»¯ thÃ´ng tin â€œngáº¯n háº¡nâ€ tá»« bÆ°á»›c trÆ°á»›c\n",
    "    - Æ¯u Ä‘iá»ƒm:\n",
    "        - Dá»… cÃ i Ä‘áº·t, concept Ä‘Æ¡n giáº£n  \n",
    "        - Xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»±\n",
    "    - NhÆ°á»£c Ä‘iá»ƒm:\n",
    "        - KhÃ³ há»c long-term dependencies (dá»¯ liá»‡u quÃ¡ dÃ i)\n",
    "        - Dá»… bá»‹ vanishing gradient\n",
    "        - Vanishing gradient = hiá»‡n tÆ°á»£ng gradient trá»Ÿ nÃªn quÃ¡ nhá» khi lan truyá»n ngÆ°á»£c (backpropagation) qua nhiá»u layer hoáº·c nhiá»u bÆ°á»›c thá»i gian, dáº«n Ä‘áº¿n viá»‡c máº¡ng há»c cháº­m hoáº·c khÃ´ng há»c Ä‘Æ°á»£c thÃ´ng tin dÃ i háº¡n.\n",
    "- DNN (Deep Neural Network) - Máº¡ng sÃ¢u\n",
    "    - Cáº¥u trÃºc:\n",
    "        - Gá»“m nhiá»u layer fully connected (dense layer)\n",
    "        - Input â†’ hidden layers â†’ output\n",
    "        - KhÃ´ng cÃ³ káº¿t ná»‘i â€œquay láº¡iâ€ giá»¯a cÃ¡c bÆ°á»›c\n",
    "    - Dá»¯ liá»‡u xá»­ lÃ½:\n",
    "        - Static input (khÃ´ng theo chuá»—i)\n",
    "        - VÃ­ dá»¥: dá»± Ä‘oÃ¡n giÃ¡ nhÃ  dá»±a trÃªn sá»‘ phÃ²ng, diá»‡n tÃ­ch, vá»‹ trÃ­\n",
    "    - Æ¯u Ä‘iá»ƒm:\n",
    "        - Máº¡nh cho cÃ¡c bÃ i toÃ¡n classification, regression vá»›i dá»¯ liá»‡u cá»‘ Ä‘á»‹nh\n",
    "        - Huáº¥n luyá»‡n Ä‘Æ¡n giáº£n hÆ¡n RNN\n",
    "    - NhÆ°á»£c Ä‘iá»ƒm:\n",
    "        - KhÃ´ng xá»­ lÃ½ tá»‘t sequence / dá»¯ liá»‡u theo thá»i gian\n",
    "        - KhÃ´ng giá»¯ tráº¡ng thÃ¡i (state) tá»« bÆ°á»›c trÆ°á»›c\n",
    "- LSTM (Long Short-Term Memory)\n",
    "    - LÃ  phiÃªn báº£n cáº£i tiáº¿n cá»§a RNN, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giá»¯ thÃ´ng tin lÃ¢u hÆ¡n.\n",
    "    - CÃ³ gate (cá»•ng) Ä‘á»ƒ quyáº¿t Ä‘á»‹nh:\n",
    "        - CÃ¡i gÃ¬ nÃªn lÆ°u (input gate)\n",
    "        - CÃ¡i gÃ¬ nÃªn quÃªn (forget gate)\n",
    "        - CÃ¡i gÃ¬ nÃªn xuáº¥t (output gate)\n",
    "    - Æ¯u Ä‘iá»ƒm:\n",
    "        - Giá»¯ Ä‘Æ°á»£c thÃ´ng tin dÃ i háº¡n\n",
    "        - Giáº£i quyáº¿t váº¥n Ä‘á» vanishing gradient cá»§a RNN\n",
    "    - NhÆ°á»£c Ä‘iá»ƒm:\n",
    "        - Phá»©c táº¡p hÆ¡n RNN, tÃ­nh toÃ¡n nhiá»u hÆ¡n\n",
    "- GRU (Gated Recurrent Unit)\n",
    "    - LÃ  biáº¿n thá»ƒ nháº¹ hÆ¡n LSTM, káº¿t há»£p forget + input gate thÃ nh update gate.\n",
    "    - Ãt tham sá»‘ hÆ¡n LSTM â†’ huáº¥n luyá»‡n nhanh hÆ¡n\n",
    "    - Æ¯u Ä‘iá»ƒm:\n",
    "        - Tá»‘t cho dá»¯ liá»‡u tuáº§n tá»± ngáº¯n vÃ  trung bÃ¬nh\n",
    "        - Nhanh hÆ¡n LSTM\n",
    "        - Hiá»‡u nÄƒng tÆ°Æ¡ng Ä‘Æ°Æ¡ng LSTM nhiá»u trÆ°á»ng há»£p\n",
    "---- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a5dce",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "1. HÃ m sigmoid: \n",
    "2. HÃ m tanh: ÄÃ¢y lÃ  má»™t hÃ m phi tuyáº¿n (non-linear) biáº¿n Ä‘á»•i giÃ¡ trá»‹ Ä‘áº§u vÃ o thÃ nh giÃ¡ trá»‹ Ä‘áº§u ra náº±m trong khoáº£ng (âˆ’1,1).\n",
    "$$\n",
    "\\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "- á»¨ng dá»¥ng\n",
    "    - Trong máº¡ng nÆ¡-ron nhÃ¢n táº¡o, hÃ m tanh thÆ°á»ng dÃ¹ng lÃ m hÃ m kÃ­ch hoáº¡t (activation function) nhá» kháº£ nÄƒng chuáº©n hÃ³a dá»¯ liá»‡u vÃ o khoáº£ng [âˆ’1,1], giÃºp quÃ¡ trÃ¬nh há»c nhanh hÆ¡n so vá»›i hÃ m sigmoid (náº±m trong khoáº£ng [0,1]).\n",
    "\n",
    "    - Trong xá»­ lÃ½ tÃ­n hiá»‡u, giÃºp chuyá»ƒn Ä‘á»•i tÃ­n hiá»‡u tuyáº¿n tÃ­nh thÃ nh phi tuyáº¿n Ä‘á»ƒ mÃ´ phá»ng cÃ¡c há»‡ thá»‘ng phi tuyáº¿n.\n",
    "3. Relu: Rectified Linear Unit: ÄÆ¡n giáº£n vÃ  hiá»‡u quáº£: TÃ­nh toÃ¡n nhanh, giÃºp giáº£m váº¥n Ä‘á» vanishing gradient (gradient biáº¿n máº¥t) thÆ°á»ng gáº·p vá»›i sigmoid hoáº·c tanh.\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "- Náº¿u ğ‘¥> 0 Ä‘áº§u ra = ğ‘¥\n",
    "- Náº¿u ğ‘¥ â‰¤ 0 Ä‘áº§u ra = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee95061",
   "metadata": {},
   "source": [
    "### Epoch\n",
    "- Epoch: Má»™t epoch lÃ  má»™t vÃ²ng láº·p Ä‘áº§y Ä‘á»§ qua toÃ n bá»™ táº­p dá»¯ liá»‡u huáº¥n luyá»‡n.\n",
    "    - Náº¿u báº¡n cÃ³ 1.000 máº«u dá»¯ liá»‡u vÃ  mÃ´ hÃ¬nh xem háº¿t cáº£ 1.000 máº«u má»™t láº§n, thÃ¬ Ä‘Ã³ lÃ  1 epoch.\n",
    "    - Sau má»—i epoch, mÃ´ hÃ¬nh cáº­p nháº­t trá»ng sá»‘ (weights) dá»±a trÃªn lá»—i (loss) tÃ­nh Ä‘Æ°á»£c tá»« toÃ n bá»™ dá»¯ liá»‡u.\n",
    "- Batch: Dá»¯ liá»‡u thÆ°á»ng Ä‘Æ°á»£c chia thÃ nh cÃ¡c pháº§n nhá» gá»i lÃ  batch. Má»™t batch Ä‘i qua mÃ´ hÃ¬nh lÃ  má»™t bÆ°á»›c (step).\n",
    "- Iterations (sá»‘ bÆ°á»›c láº·p): Náº¿u báº¡n cÃ³ 1.000 máº«u dá»¯ liá»‡u vÃ  batch size = 100, thÃ¬ má»™t epoch sáº½ gá»“m 10 iterations.\n",
    "- Vai trÃ² cá»§a epoch: \n",
    "    - Epoch quÃ¡ tháº¥p â†’ mÃ´ hÃ¬nh chÆ°a há»c Ä‘á»§, káº¿t quáº£ kÃ©m (underfitting).\n",
    "    - Epoch quÃ¡ cao â†’ mÃ´ hÃ¬nh há»c quÃ¡ ká»¹ dá»¯ liá»‡u huáº¥n luyá»‡n, máº¥t kháº£ nÄƒng tá»•ng quÃ¡t (overfitting)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
