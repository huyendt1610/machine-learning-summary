{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caff7a15",
   "metadata": {},
   "source": [
    "### Machine learning \n",
    "- SciPy: Scientific Python, Python library for scientific and technical computing, built on top of NumPy\n",
    "- Scikit-image: SciPy toolkit for image\n",
    "- Scikit-learn: SciPy toolkit for machine learning (sklearn)\n",
    "\n",
    "Type of machine learning:\n",
    "1. By learning style:\n",
    "    - Supervised Learning (Học có giám sát)\n",
    "        - Classification (Phân loại): fixed group, find data thuoc ve nhom nao\n",
    "        - Regression (Hồi quy): linear regression\n",
    "    - Unsupervised Learning (Học không giám sát)\n",
    "        - Clustering (phân nhóm)\n",
    "        - Association\n",
    "    - Semi-Supervised Learning (Học bán giám sát)\n",
    "    - Reinforcement Learning (Học Củng Cố)\n",
    "2. Phân nhóm dựa trên chức năng\n",
    "    - Regression Algorithms\n",
    "        - Linear Regression: **ok**\n",
    "        - Logistic Regression: both linear regression and sigmoid (ham sigmoid tuan theo logic function dang chu S)\n",
    "        - Stepwise Regression:\n",
    "        - PLA: (perception Learning Algorithm)\n",
    "            - Chọn ngẫu nhiên một vector hệ số w với các phần tử gần 0.\n",
    "            - Duyệt ngẫu nhiên qua từng điểm dữ liệu xi:\n",
    "            - Nếu xi được phân lớp đúng, tức sgn(wT * xi)=yi, chúng ta không cần làm gì.\n",
    "            - Nếu xi bị misclassifed, cập nhật w theo công thức: w = w + yi * xi\n",
    "            - Kiểm tra xem có bao nhiêu điểm bị misclassifed. Nếu không còn điểm nào, dừng thuật toán. Nếu còn, quay lại bước 2.\n",
    "\n",
    "            * Một cách tự nhiên, nếu có một vài nhiễu, ta sẽ đi tìm một đường thẳng phân chia hai class sao cho có ít điểm bị misclassified nhất. Việc này có thể được thực hiện thông qua PLA với một chút thay đổi nhỏ như sau:\n",
    "\n",
    "                - Giới hạn số lượng vòng lặp của PLA.\n",
    "                - Mỗi lần cập nhật nghiệm w mới, ta đếm xem có bao nhiêu điểm bị misclassified. Nếu là lần đầu tiên, giữ lại nghiệm này trong pocket (túi quần). Nếu không, so sánh số điểm misclassified này với số điểm misclassified của nghiệm trong pocket, nếu nhỏ hơn thì lôi nghiệm cũ ra, đặt nghiệm mới này vào.\n",
    "    - Classification Algorithms\n",
    "        - Linear Classifier\n",
    "        - Support Vector Machine (SVM): tìm ra 1 đg thẳng tối ưu có thể phân tách dữ liệu về các classes khác nhau trong không gian đa chiều\n",
    "            - đường này cách đều các điểm dữ liệu gần nhất \n",
    "            - marin là tối đa\n",
    "            - các điểm dữ liệu thuộc về các classes mà gần với các đường phân cách nhất được gọi là Support Vector\n",
    "            - các đường phân cách là Hyperplane (siêu phẳng)\n",
    "            - đường phân cách tối ưu là Optimal Hyperplane\n",
    "        - Kernel SVM\n",
    "        - Sparse Representation-based classification (SRC)\n",
    "    - Instance-based Algorithms\n",
    "        - k-Nearest Neighbor (kNN):\n",
    "            - ko có huấn luyện trước\n",
    "            - khi có 1 điểm dữ liệu mới, tìm K thằng gần nó nhất (K thường là số lẻ)\n",
    "            - xét K điểm này đa số thuộc class nào -> gán cho điểm dữ liệu mới\n",
    "            (nếu là K-Nearest Regression -> dùng trung bình của K điểm cho điểm mới)\n",
    "\n",
    "            - sẽ chậm nếu dữ liệu có nhiều data points\n",
    "        - Learning Vector Quantization (LVQ)\n",
    "    - Regularization Algorithms: (chính quy hóa) thuật toán kiểm soát độ sai số của mô hình và độ phức tạp của mô hình (phức tạp quá có thể bị overfitting) <br>\n",
    "        Cost function = Loss + L2 Weight Penalty (regularization term)\n",
    "        + thường thì để loss ít nhất thì các trọng số Wi có giá trị lớn -> mô hình có độ phức tạp (hàm mũ lớn)\n",
    "        + Phần còn lại L2 sẽ đối nghịch phần 1 để cân bằng lại giá trị cost\n",
    "\n",
    "        - Least Absolute Shrinkage and Selection Operator (LASSO) (L1 Regularization): regularization term = sum(abs(wi)) \n",
    "            - ép cho 1 số trọng số về giá trị 0 -> các biến tương ứng ko ảnh hưởng kết quả nữa\n",
    "            - thường đc sử dụng trong feature selection (chọn lựa đặc trưng)\n",
    "        - Ridge Regression (L2 regularization): regularization term = sum(wi^2)\n",
    "            - có xu hướng làm giảm độ lớn của các trọng số\n",
    "            - làm giảm độ ảnh howngr của các feature tới kq cuối cùng nhưng ko triệt tiêu bất kỳ feature nào\n",
    "        - Elastic Net (L1 + L2)\n",
    "        - Least-Angle Regression (LARS)\n",
    "\n",
    "    - Bayesian Algorithms **gia su tinh doc lap giua cac bien**\n",
    "        - Gaussian Naive Bayes: **dữ liệu là các biến liên tục tuân theo phân phối chuẩn Gaussian**\n",
    "         (Khi các đặc trưng nhận giá trị liên tục, ta giả sử các đặc trưng đó có phân phối Gaussian. Khi đó, likelihood sẽ có dạng: f(x) = f(mean, phuong sai))\n",
    "            - Tính giá trị f(x) theo mean, và phương sai của mỗi cluster (class)\n",
    "            - cluster nào có giá trị f(x) = p(max) thì x sẽ thuộc class đó\n",
    "            - Nếu x là X với nhiều features => f(x) = f(x1|c) * f(x2|c) *... * p(c)\n",
    "            vd> đi ra ngoài = P (f (thời tiết) | class = đi ra ngoài) * P (f(xe hơi) | class = đi ra ngoài) * P (class = đi ra ngoài)\n",
    "            - Mỗi điểm thuộc về 1 cluster nếu nó gần cluster ấy nhất\n",
    "        - Multinomial Naive Bayes: **su dung cho phan loai van ban wordA: 1, wordB: 2**\n",
    "        - Bernoulli Naive: **cung dung tron phan loai van ban wordA: 0, wordB: 1**\n",
    "\n",
    "    - Clustering Algorithms\n",
    "        - k-Means clustering: \n",
    "            - chọn ngẫu nhiên K phần tử làm trung tâm của các clusters, \n",
    "            - tính khoảng cách Ecluid từ các điểm tới các K centeroids này, \n",
    "            - rồi dựa vào khoảng cách tới centers nào nhỏ nhất thì điểm dữ liệu thuộc class đó\n",
    "            - vòng 2 tính lại centers của mỗi nhóm theo phân loại nhóm ở bước trên\n",
    "            - lặp lại tới khi phân loại nhóm cho từng điểm ko thay đổi nữa\n",
    "\n",
    "            - ưu: đơn giản\n",
    "            - nhược: \n",
    "                - khó khăn khi tìm K \n",
    "                - khởi tạo các điểm trung tâm ko tốt -> kết quả ko tốt và hội tụ chậm \n",
    "        - Agglomerative Clustering: (Hierarchical Clustering)\n",
    "            - gom 2 điểm thành cụm {A, B}\n",
    "            - lấy 1 điểm mới D -> tính khoảng cách eucliden từ D tới các điểm khác và tới tâm của  {A, B}\n",
    "            - gán D về với cụm mà khoảng cách ngắn nhất (Có thể là {D, E} hoặc  {A, B, D})\\\n",
    "            - lặp lại với các điểm khác \n",
    "\n",
    "            - Khoảng cách giữa 2 cụm chính là điểm khác biệt giữa chúng. Khi chọn 2 cụm để hợp nhất ở mỗi bước, cách chọn \"2 cụm nào hợp nhất\" phụ thuộc vào cách tính khoảng cách giữa chúng và đều là chọn \"khoảng cách ngắn nhất\". Có những phương pháp giúp xác định khoảng cách giữa hai cụm như sau:\n",
    "                - ward: \n",
    "                    - Ward không tính trực tiếp khoảng cách giữa điểm mà tính khoảng cách giữa các cụm dựa trên tổng bình phương sai lệch (sum of squared differences).\n",
    "                    - Khi hợp nhất hai cụm, Ward chọn cặp cụm sao cho \"tăng tổng phương sai\" trong các cụm là nhỏ nhất.\n",
    "                    - hợp nhất các cụm sao cho tổng phương sai bên trong cụm tăng ít nhất.\n",
    "                - single: Khoảng cách giữa hai cụm: là khoảng cách nhỏ nhất giữa một điểm trong cụm này và một điểm trong cụm kia.\n",
    "                - complete: Khoảng cách giữa hai cụm: là khoảng cách lớn nhất giữa các điểm của hai cụm.\n",
    "                - average: Khoảng cách giữa hai cụm: là trung bình khoảng cách giữa tất cả các cặp điểm thuộc hai cụm.\n",
    "        - DBSCAN: Density-Based Clustering (Density-Based Spatial Clustering of Applications with Noise): phân cụm dựa trên mật độ cao thấp. Có khả năng phát hiện các cụm có hình dạng bất kỳ và xử lý được các điểm nhiễu (outliers).\n",
    "            - DBSCAN dựa trên mật độ điểm xung quanh một điểm dữ liệu:\n",
    "            - Một cụm hình thành từ các vùng có mật độ điểm cao.\n",
    "            - Các điểm nằm ngoài vùng mật độ cao được coi là nhiễu (noise / outliers).\n",
    "\n",
    "            - Các tham số chính: \n",
    "                - eps (epsilon): Khoảng cách tối đa giữa hai điểm để được coi là “hàng xóm” (neighbor).\n",
    "                - minPts (minimum points): Số điểm tối thiểu trong vùng bán kính eps để coi điểm là core point (điểm trung tâm của cụm).\n",
    "            - Phân loại điểm: \n",
    "                - Core point (điểm trung tâm): Có ≥ minPts điểm trong bán kính eps.\n",
    "                - Border point (điểm biên): Có < minPts điểm trong eps, nhưng nằm trong vùng eps của core point.\n",
    "                - Noise (điểm nhiễu): Không phải core point, không phải border point.\n",
    "            - Quy trình: \n",
    "                - Chọn một điểm chưa gán cụm.\n",
    "                - Nếu là core point, tạo cụm mới và thêm tất cả các điểm lân cận (eps) vào cụm, mở rộng cụm bằng cách lặp lại với các core point mới.\n",
    "                - Nếu là border point, gán vào cụm của core point liên kết.\n",
    "                - Nếu không thuộc loại nào → đánh dấu là noise.\n",
    "                - Lặp lại cho đến khi tất cả các điểm được gán cụm hoặc là noise.\n",
    "            - Nhược điểm\n",
    "                - Nhạy với thông số eps và minPts.\n",
    "                - Không hiệu quả khi mật độ các cụm khác nhau nhiều.\n",
    "        - k-Medians\n",
    "        - Expectation Maximization (EM)\n",
    "\n",
    "    - Artificial Neural Network Algorithms\n",
    "        - Perceptron\n",
    "        - Softmax Regression\n",
    "        - Multi-layer Perceptron\n",
    "        - Back-Propagation\n",
    "\n",
    "    - Dimensionality Reduction Algorithms: thuật toán giảm chiều dữ liệu để trực quan hóa dữ liệu\n",
    "        - Principal Component Analysis (PCA): \n",
    "            - biến đổi từ không gian gốc ban đầu sang không gian mới với trục mới sao cho trong không gian mới này độ phân tán dữ liệu là tối đa theo các trục \n",
    "            - Sắp xếp các trục trong chiều không gian mới này theo thứ tự giảm dần của độ phân tán dữ liệu\n",
    "            - Chọn và giữ lại các trục có độ phân tán của dữ liệu là lớn nhất \n",
    "        - Linear Discriminant Analysis (LDA)\n",
    "\n",
    "    - Ensemble Algorithms: kết hợp nhiều mô hình nhỏ, độ chính xác thấp -> mô hình lớn, độ chính xác cao\n",
    "        - Bagging: Random Forest là dạng này, \n",
    "            - kết hợp mô hình con song song, sau đó lấy majority vote cho classification or average cho regression\n",
    "            - mô hình con đc huấn luyện trên các tập con của tập dữ liệu\n",
    "            - mô hình con là cùng 1 loại (decision tree, SVM, logistic regression or...)\n",
    "        - Boosting: - AdaBoost là một trong các dạng này  \n",
    "            - các mô hình con đc chạy tuần tự\n",
    "            - mô hình con đc huấn luyện trên toàn bộ tập dữ liệu\n",
    "            - mô hình sau tập trung vào các điểm dữ liệu mà mô hình trước dự đoán sai \n",
    "                - gán trọng số cao hơn cho các điểm mà mô hình trước dự đoán sai\n",
    "                - và ngc lại gán trọng số thấp hơn cho các điểm đã đc dự đoán đúng\n",
    "            - kết quả cuối cùng thì vẫn là majority vote or average các dự đoán của các mô hình con \n",
    "            - Nhưng avg sẽ có trọng số, càng về sau các mô hình chính xác hơn thì sẽ có trọng số cao hơn \n",
    "        - Voting:\n",
    "            - giống Bagging là cho các mô hình con chạy song song nhưng mô hình con là các loại khác nhau\n",
    "            - mô hình con đc trained cùng 1 tập dữ liệu\n",
    "            + hard voting: kết quả cuối theo majority vote \n",
    "            + soft voting: \n",
    "                - kết quả của các mô hình con là dạng sx: a% class1, b% class2\n",
    "                - tính tổng % của mỗi class -> class nào có tổng % lớn hơn sẽ đc chọn là kết quả\n",
    "        - Stacking: nhiều model đc chia làm 2 nhóm\n",
    "            - nhóm 1: base models: gồm nhiều model khác nhau\n",
    "            - nhóm 2: meta model: chỉ có 1 model\n",
    "\n",
    "            - chia dữ liệu thành 2 bộ: train và validation \n",
    "            - huấn luyện toàn bộ base models với train data\n",
    "            - đem base models đi dự đoán cho bộ validation \n",
    "            - các dự đoán của base models sẽ đc dùng làm dữ liệu train cho meta models\n",
    "\n",
    "            - ưu: chính xác hơn\n",
    "            - nhược:\n",
    "                - chậm vì phải train các model con \n",
    "                - độ giải thích thấp\n",
    "        \n",
    "        - Random Forest: \n",
    "            - sử dụng nhiều tree khác nhau -> kết quả của đầu ra là max của majority vote của các tree con này\n",
    "            - các tree con được huấn luyện dựa vào các tập dữ liệu con khác nhau: theo thuật toán bootrapping> lấy mẫu có hoàn lại (kiểu tổ hợp khác nhau từ dữ liệu ban đầu)\n",
    "            - việc lựa chọn phân nhánh theo feature nào trước dựa vào gini value: giá trị này càng nhỏ thì tính phân loại càng tốt vì Gini = 1 - sum(pi)\n",
    "        \n",
    "        - Và còn rất nhiều các thuật toán khác."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9929459",
   "metadata": {},
   "source": [
    "- MLE: Maximum likelihood estimation: tìm tham số **(theta)** cho modeling chỉ dựa vào training data\n",
    "- MAP: Maximum a posteriori: cả training data và cảm quan của người xây dựng mô hình "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
